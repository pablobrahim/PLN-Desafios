{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Desafio 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJgf6GQIIEH1"
   },
   "source": [
    "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
    "Estudiar los 5 documentos mÃ¡s similares de cada uno analizar si tiene sentido\n",
    "la similaridad segÃºn el contenido del texto y la etiqueta de clasificaciÃ³n.\n",
    "\n",
    "**2**. Construir un modelo de clasificaciÃ³n por prototipos (tipo zero-shot). Clasificar los documentos de un conjunto de test comparando cada uno con todos los de entrenamiento y asignar la clase al label del documento del conjunto de entrenamiento con mayor similaridad.\n",
    "\n",
    "**3**. Entrenar modelos de clasificaciÃ³n NaÃ¯ve Bayes para maximizar el desempeÃ±o de clasificaciÃ³n\n",
    "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parÃ¡mteros\n",
    "de instanciaciÃ³n del vectorizador y los modelos y probar modelos de NaÃ¯ve Bayes Multinomial\n",
    "y ComplementNB.\n",
    "\n",
    "**4**. Transponer la matriz documento-tÃ©rmino. De esa manera se obtiene una matriz\n",
    "tÃ©rmino-documento que puede ser interpretada como una colecciÃ³n de vectorizaciÃ³n de palabras.\n",
    "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 mÃ¡s similares. La elecciÃ³n de palabras no debe ser al azar para evitar la apariciÃ³n de tÃ©rminos poco interpretables, elegirlas \"manualmente\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos de entrenamiento luego de limpieza: 10892\n",
      "Documentos de test luego de limpieza: 7226\n",
      "Cantidad de documentos de entrenamiento: 10892\n",
      "Cantidad de documentos de test: 7226\n",
      "Cantidad de clases: 20\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Carga del dataset\n",
    "# --------------------------------------\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "test  = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X_train = train.data\n",
    "y_train = train.target\n",
    "X_test  = test.data\n",
    "y_test  = test.target\n",
    "target_names = train.target_names\n",
    "\n",
    "# --- ðŸ”¹ Limpiar documentos vacÃ­os ---\n",
    "def limpiar_docs(textos, etiquetas):\n",
    "    X_limpio, y_limpio = [], []\n",
    "    for x, y in zip(textos, etiquetas):\n",
    "        x_clean = (x or \"\").strip()\n",
    "        if len(x_clean) > 30:\n",
    "            X_limpio.append(x_clean)\n",
    "            y_limpio.append(y)\n",
    "    return X_limpio, y_limpio\n",
    "\n",
    "X_train, y_train = limpiar_docs(X_train, y_train)\n",
    "X_test, y_test = limpiar_docs(X_test, y_test)\n",
    "\n",
    "print(f\"Documentos de entrenamiento luego de limpieza: {len(X_train)}\")\n",
    "print(f\"Documentos de test luego de limpieza: {len(X_test)}\")\n",
    "\n",
    "print(f\"Cantidad de documentos de entrenamiento: {len(X_train)}\")\n",
    "print(f\"Cantidad de documentos de test: {len(X_test)}\")\n",
    "print(f\"Cantidad de clases: {len(target_names)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Documento Ã­ndice 12623 (test) â€” etiqueta: misc.forsale\n",
      "Extracto: Sharp brand \"Pocket Computer\" model PC-1246     Dimensions;  3.5 x 5 x 0.5 inches.          Has 15-digit LCD display         53 rubber keys (w/alphabet)         built-in BASIC prog.language         an \n",
      "\n",
      "  â€¢ idx  4026 | train | label=rec.autos                 | similitud=0.1690\n",
      "  â€¢ idx   370 | train | label=sci.electronics           | similitud=0.1644\n",
      "  â€¢ idx 12058 | test  | label=comp.graphics             | similitud=0.1570\n",
      "  â€¢ idx 11436 | test  | label=comp.sys.ibm.pc.hardware  | similitud=0.1494\n",
      "  â€¢ idx  8680 | train | label=misc.forsale              | similitud=0.1458\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Documento Ã­ndice 13781 (test) â€” etiqueta: misc.forsale\n",
      "Extracto: Hallo all...my girlfriend and I will be travelling across the US this summer, so we won't be using our tickets to return to Hawaii.  Please buy them.  The tickets are one-way, leaving Peoria, IL on Ma \n",
      "\n",
      "  â€¢ idx   108 | train | label=misc.forsale              | similitud=0.2557\n",
      "  â€¢ idx  2277 | train | label=rec.sport.hockey          | similitud=0.2251\n",
      "  â€¢ idx 16599 | test  | label=rec.sport.baseball        | similitud=0.2090\n",
      "  â€¢ idx 17206 | test  | label=misc.forsale              | similitud=0.1992\n",
      "  â€¢ idx 17875 | test  | label=misc.forsale              | similitud=0.1992\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Documento Ã­ndice 1326 (train) â€” etiqueta: talk.politics.mideast\n",
      "Extracto: Mr. Emmanuel Huna,  Give logic a break will you.  Gosh, what kind of intelligence do you have, if any?   Tesiel says :  Be a man not an arab for once. I say       :  Fuck of Tsiel (for saying the abov \n",
      "\n",
      "  â€¢ idx  4678 | train | label=talk.politics.mideast     | similitud=0.2026\n",
      "  â€¢ idx  1164 | train | label=rec.sport.hockey          | similitud=0.1795\n",
      "  â€¢ idx  7513 | train | label=rec.sport.hockey          | similitud=0.1639\n",
      "  â€¢ idx  9224 | train | label=talk.politics.misc        | similitud=0.1458\n",
      "  â€¢ idx 11696 | test  | label=talk.politics.mideast     | similitud=0.1260\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Documento Ã­ndice 8484 (train) â€” etiqueta: rec.autos\n",
      "Extracto: The Olds Supreme Convertible got high marks in C/D's recent test, if you can get by the stupid body moldings and stuff.  The Saab 900 ragtop may be out of your range, but its a good choice.  Is there  \n",
      "\n",
      "  â€¢ idx 18089 | test  | label=rec.autos                 | similitud=0.4621\n",
      "  â€¢ idx  4828 | train | label=rec.autos                 | similitud=0.4275\n",
      "  â€¢ idx 14020 | test  | label=rec.autos                 | similitud=0.4017\n",
      "  â€¢ idx  5608 | train | label=rec.autos                 | similitud=0.2872\n",
      "  â€¢ idx  1011 | train | label=rec.autos                 | similitud=0.2846\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Documento Ã­ndice 16753 (test) â€” etiqueta: misc.forsale\n",
      "Extracto: I have the following Marx Brothers tapes forsale. I would like to sell them as a batch if possible. All (except *) are new, carfully stored copies I bought. I now own the laserdisks.  MGM/UA: A Day at \n",
      "\n",
      "  â€¢ idx 17246 | test  | label=misc.forsale              | similitud=0.2985\n",
      "  â€¢ idx 16497 | test  | label=misc.forsale              | similitud=0.2776\n",
      "  â€¢ idx 15745 | test  | label=misc.forsale              | similitud=0.2420\n",
      "  â€¢ idx 16336 | test  | label=misc.forsale              | similitud=0.2234\n",
      "  â€¢ idx  8171 | train | label=talk.religion.misc        | similitud=0.1519\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# 1. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
    "#    Estudiar los 5 documentos mÃ¡s similares de cada uno analizar si tiene sentido la similaridad segÃºn el \n",
    "#    contenido del texto y la etiqueta de clasificaciÃ³n\n",
    "# --------------------------------------\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "X_all = vectorizer.fit_transform(X_train + X_test)\n",
    "all_data = X_train + X_test\n",
    "n_train = len(X_train)\n",
    "\n",
    "# Seleccionamos 5 documentos al azar\n",
    "random.seed(0)\n",
    "indices = random.sample(range(X_all.shape[0]), 5)\n",
    "\n",
    "for idx in indices:\n",
    "    sim = cosine_similarity(X_all[idx], X_all).ravel()\n",
    "    sim[idx] = 0  # Evitar el mismo documento\n",
    "    top5 = sim.argsort()[-5:][::-1]\n",
    "\n",
    "    # Determinar conjunto y etiqueta del documento original\n",
    "    if idx < n_train:\n",
    "        label = target_names[y_train[idx]]\n",
    "        subset = \"train\"\n",
    "    else:\n",
    "        label = target_names[y_test[idx - n_train]]\n",
    "        subset = \"test\"\n",
    "\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"Documento Ã­ndice {idx} ({subset}) â€” etiqueta: {label}\")\n",
    "    print(\"Extracto:\", all_data[idx][:200].replace(\"\\n\", \" \"), \"\\n\")\n",
    "\n",
    "    for j in top5:\n",
    "        if j < n_train:\n",
    "            lab = target_names[y_train[j]]\n",
    "            subset_j = \"train\"\n",
    "        else:\n",
    "            lab = target_names[y_test[j - n_train]]\n",
    "            subset_j = \"test\"\n",
    "        print(f\"  â€¢ idx {j:5d} | {subset_j:5s} | label={lab:25s} | similitud={sim[j]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El anÃ¡lisis de similaridad utilizando la representaciÃ³n TF-IDF muestra resultados coherentes en la mayorÃ­a de los casos.\n",
    "Por ejemplo, los documentos con etiqueta misc.forsale presentan como mÃ¡s similares otros textos tambiÃ©n pertenecientes a esa categorÃ­a, lo que tiene sentido porque contienen tÃ©rminos asociados a ventas (â€œfor saleâ€, â€œbuyâ€, â€œticketsâ€, â€œbatchâ€, â€œcopiesâ€, â€œpriceâ€, etc.).\n",
    "TambiÃ©n se observa que, cuando los documentos tratan de temas mÃ¡s tÃ©cnicos o especÃ­ficos â€”como rec.autos o talk.politics.mideastâ€”, las similitudes altas suelen darse con documentos de la misma clase o de clases temÃ¡ticamente cercanas.\n",
    "Igualmente, aparecen algunos casos en los que documentos de distintas categorÃ­as muestran similitudes no triviales. Esto se debe a que la representaciÃ³n TF-IDF no captura el contexto semÃ¡ntico profundo, sino la coocurrencia de tÃ©rminos. Entonces, palabras mas genÃ©ricas pueden estar presentes en mÃºltiples categorÃ­as tÃ©cnicas, afectando la medida de similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLASIFICADOR POR PROTOTIPOS\n",
      "Accuracy: 0.5569\n",
      "F1 macro: 0.5492\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# 2. Construir un modelo de clasificaciÃ³n por prototipos (tipo zero-shot). Clasificar los documentos de un conjunto\n",
    "#    de test comparando cada uno con todos los de entrenamiento y asignar la clase al label del documento del conjunto\n",
    "#    de entrenamiento con mayor similaridad. \n",
    "# --------------------------------------\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec  = vectorizer.transform(X_test)\n",
    "\n",
    "y_pred_proto = []\n",
    "for i in range(X_test_vec.shape[0]):\n",
    "    sims = cosine_similarity(X_test_vec[i], X_train_vec).ravel()\n",
    "    best = np.argmax(sims)\n",
    "    y_pred_proto.append(y_train[best])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASIFICADOR POR PROTOTIPOS\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_proto):.4f}\")\n",
    "print(f\"F1 macro: {f1_score(y_test, y_pred_proto, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo tipo â€œzero-shotâ€ basado en comparaciÃ³n de similaridad con los documentos de entrenamiento alcanza un accuracy de 0.56 y un F1 macro de 0.55, lo cual es razonable considerando su sencillez y la naturaleza no supervisada del enfoque.\n",
    "En este tipo de modelo, cada documento de test se asigna a la clase del documento mÃ¡s similar en entrenamiento. Esto implica que:\n",
    "Es muy sensible al \"ruido\" en los textos.\n",
    "No aprovecha la distribuciÃ³n global de las clases, sino que se apoya en un Ãºnico ejemplo.\n",
    "Depende fuertemente de la calidad de la representaciÃ³n vectorial.\n",
    "El rendimiento obtenido sugiere que la representaciÃ³n TF-IDF logra capturar cierta estructura temÃ¡tica, pero no lo suficiente como para generalizar bien a nuevos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODELO: CountVectorizer + MultinomialNB\n",
      "Accuracy: 0.6742\n",
      "F1 macro: 0.6408\n",
      "\n",
      "Reporte de clasificaciÃ³n (resumen):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.58      0.48      0.53       306\n",
      "           comp.graphics       0.55      0.73      0.63       380\n",
      " comp.os.ms-windows.misc       0.33      0.00      0.01       378\n",
      "comp.sys.ibm.pc.hardware       0.51      0.75      0.60       382\n",
      "   comp.sys.mac.hardware       0.68      0.66      0.67       369\n",
      "          comp.windows.x       0.63      0.77      0.70       382\n",
      "            misc.forsale       0.84      0.74      0.79       377\n",
      "               rec.autos       0.78      0.77      0.77       369\n",
      "         rec.motorcycles       0.85      0.73      0.79       378\n",
      "      rec.sport.baseball       0.92      0.82      0.87       374\n",
      "        rec.sport.hockey       0.94      0.87      0.90       387\n",
      "               sci.crypt       0.65      0.80      0.72       371\n",
      "         sci.electronics       0.67      0.53      0.59       379\n",
      "                 sci.med       0.83      0.83      0.83       379\n",
      "               sci.space       0.78      0.77      0.78       376\n",
      "  soc.religion.christian       0.52      0.91      0.66       384\n",
      "      talk.politics.guns       0.57      0.69      0.62       347\n",
      "   talk.politics.mideast       0.74      0.77      0.76       366\n",
      "      talk.politics.misc       0.43      0.45      0.44       301\n",
      "      talk.religion.misc       0.37      0.10      0.16       241\n",
      "\n",
      "                accuracy                           0.67      7226\n",
      "               macro avg       0.66      0.66      0.64      7226\n",
      "            weighted avg       0.67      0.67      0.65      7226\n",
      "\n",
      "\n",
      "======================================================================\n",
      "MODELO: TFIDF + MultinomialNB\n",
      "Accuracy: 0.7050\n",
      "F1 macro: 0.6740\n",
      "\n",
      "Reporte de clasificaciÃ³n (resumen):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.74      0.20      0.31       306\n",
      "           comp.graphics       0.66      0.71      0.69       380\n",
      " comp.os.ms-windows.misc       0.68      0.61      0.64       378\n",
      "comp.sys.ibm.pc.hardware       0.60      0.76      0.67       382\n",
      "   comp.sys.mac.hardware       0.79      0.69      0.74       369\n",
      "          comp.windows.x       0.79      0.78      0.78       382\n",
      "            misc.forsale       0.78      0.81      0.80       377\n",
      "               rec.autos       0.84      0.77      0.80       369\n",
      "         rec.motorcycles       0.86      0.78      0.82       378\n",
      "      rec.sport.baseball       0.94      0.85      0.89       374\n",
      "        rec.sport.hockey       0.89      0.93      0.91       387\n",
      "               sci.crypt       0.63      0.84      0.72       371\n",
      "         sci.electronics       0.70      0.55      0.62       379\n",
      "                 sci.med       0.89      0.79      0.84       379\n",
      "               sci.space       0.77      0.78      0.77       376\n",
      "  soc.religion.christian       0.38      0.96      0.54       384\n",
      "      talk.politics.guns       0.57      0.75      0.65       347\n",
      "   talk.politics.mideast       0.83      0.80      0.81       366\n",
      "      talk.politics.misc       0.87      0.32      0.46       301\n",
      "      talk.religion.misc       0.67      0.01      0.02       241\n",
      "\n",
      "                accuracy                           0.70      7226\n",
      "               macro avg       0.74      0.68      0.67      7226\n",
      "            weighted avg       0.74      0.70      0.69      7226\n",
      "\n",
      "\n",
      "======================================================================\n",
      "MODELO: TFIDF + ComplementNB\n",
      "Accuracy: 0.7382\n",
      "F1 macro: 0.7147\n",
      "\n",
      "Reporte de clasificaciÃ³n (resumen):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.60      0.42      0.49       306\n",
      "           comp.graphics       0.71      0.73      0.72       380\n",
      " comp.os.ms-windows.misc       0.70      0.65      0.67       378\n",
      "comp.sys.ibm.pc.hardware       0.64      0.71      0.68       382\n",
      "   comp.sys.mac.hardware       0.77      0.75      0.76       369\n",
      "          comp.windows.x       0.81      0.79      0.80       382\n",
      "            misc.forsale       0.77      0.76      0.76       377\n",
      "               rec.autos       0.82      0.79      0.80       369\n",
      "         rec.motorcycles       0.83      0.80      0.82       378\n",
      "      rec.sport.baseball       0.93      0.88      0.90       374\n",
      "        rec.sport.hockey       0.85      0.95      0.90       387\n",
      "               sci.crypt       0.77      0.85      0.81       371\n",
      "         sci.electronics       0.71      0.57      0.63       379\n",
      "                 sci.med       0.83      0.84      0.83       379\n",
      "               sci.space       0.79      0.84      0.81       376\n",
      "  soc.religion.christian       0.55      0.92      0.69       384\n",
      "      talk.politics.guns       0.59      0.76      0.66       347\n",
      "   talk.politics.mideast       0.81      0.86      0.83       366\n",
      "      talk.politics.misc       0.65      0.44      0.52       301\n",
      "      talk.religion.misc       0.48      0.12      0.19       241\n",
      "\n",
      "                accuracy                           0.74      7226\n",
      "               macro avg       0.73      0.72      0.71      7226\n",
      "            weighted avg       0.74      0.74      0.73      7226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# 3. Entrenar modelos de clasificaciÃ³n NaÃ¯ve Bayes para maximizar el desempeÃ±o de clasificaciÃ³n \n",
    "#    (f1-score macro) en el conjunto de datos de test. Considerar cambiar parÃ¡mteros de instanciaciÃ³n \n",
    "#    del vectorizador y los modelos y probar modelos de NaÃ¯ve Bayes Multinomial y ComplementNB.\n",
    "# --------------------------------------\n",
    "\n",
    "configs = [\n",
    "    (\"CountVectorizer + MultinomialNB\", CountVectorizer(stop_words='english', min_df=2), MultinomialNB()),\n",
    "    (\"TFIDF + MultinomialNB\", TfidfVectorizer(stop_words='english', min_df=2), MultinomialNB()),\n",
    "    (\"TFIDF + ComplementNB\", TfidfVectorizer(stop_words='english', min_df=2), ComplementNB())\n",
    "]\n",
    "\n",
    "for name, vect, model in configs:\n",
    "    Xtr = vect.fit_transform(X_train)\n",
    "    Xte = vect.transform(X_test)\n",
    "\n",
    "    model.fit(Xtr, y_train)\n",
    "    y_pred = model.predict(Xte)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"MODELO: {name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 macro: {f1m:.4f}\")\n",
    "    print(\"\\nReporte de clasificaciÃ³n (resumen):\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de los tres modelos probados muestran una mejora significativa:\n",
    "\n",
    "Modelo\tAccuracy\tF1-macro\n",
    "CountVectorizer + MultinomialNB\t0.67\t0.64\n",
    "TF-IDF + MultinomialNB\t0.71\t0.67\n",
    "TF-IDF + ComplementNB\t0.74\t0.71\n",
    "\n",
    "El mejor desempeÃ±o se obtuvo con TF-IDF + ComplementNB, lo que concuerda con la teorÃ­a: el modelo ComplementNB estÃ¡ diseÃ±ado para manejar mejor clases desbalanceadas y situaciones donde las caracterÃ­sticas mas discriminativas aparecen con baja frecuencia.\n",
    "\n",
    "El uso de TF-IDF tambiÃ©n mejora el rendimiento respecto a los conteos crudos, al ponderar las palabras mÃ¡s informativas y reducir el peso de tÃ©rminos comunes.\n",
    "Las categorÃ­as con temas mÃ¡s tÃ©cnicos y vocabularios mÃ¡s especializados (por ejemplo, rec.autos, rec.sport.hockey, sci.med, sci.space) son las mejor clasificadas, mientras que las mÃ¡s ambiguas o con contenido variado (talk.religion.misc, alt.atheism, talk.politics.misc) presentan menor desempeÃ±o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Palabra: 'space' â€” mÃ¡s similares:\n",
      "  â€¢ nasa             (similitud = 0.3179)\n",
      "  â€¢ shuttle          (similitud = 0.2784)\n",
      "  â€¢ exploration      (similitud = 0.2329)\n",
      "  â€¢ aeronautics      (similitud = 0.2221)\n",
      "  â€¢ sci              (similitud = 0.2167)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Palabra: 'computer' â€” mÃ¡s similares:\n",
      "  â€¢ shopper          (similitud = 0.1349)\n",
      "  â€¢ verlag           (similitud = 0.1248)\n",
      "  â€¢ delicate         (similitud = 0.1197)\n",
      "  â€¢ drive            (similitud = 0.1106)\n",
      "  â€¢ hackers          (similitud = 0.1082)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Palabra: 'car' â€” mÃ¡s similares:\n",
      "  â€¢ cars             (similitud = 0.1923)\n",
      "  â€¢ dealer           (similitud = 0.1773)\n",
      "  â€¢ civic            (similitud = 0.1635)\n",
      "  â€¢ loan             (similitud = 0.1561)\n",
      "  â€¢ owner            (similitud = 0.1484)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Palabra: 'engine' â€” mÃ¡s similares:\n",
      "  â€¢ household        (similitud = 0.1732)\n",
      "  â€¢ exhaust          (similitud = 0.1714)\n",
      "  â€¢ rebuilt          (similitud = 0.1706)\n",
      "  â€¢ oil              (similitud = 0.1594)\n",
      "  â€¢ diesel           (similitud = 0.1574)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Palabra: 'religion' â€” mÃ¡s similares:\n",
      "  â€¢ religious        (similitud = 0.2476)\n",
      "  â€¢ religions        (similitud = 0.2237)\n",
      "  â€¢ crusades         (similitud = 0.1937)\n",
      "  â€¢ christianity     (similitud = 0.1881)\n",
      "  â€¢ categorized      (similitud = 0.1848)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# 4. Transponer la matriz documento-tÃ©rmino. De esa manera se obtiene una matriz tÃ©rmino-documento\n",
    "#    que puede ser interpretada como una colecciÃ³n de vectorizaciÃ³n de palabras. Estudiar ahora \n",
    "#    similaridad entre palabras tomando 5 palabras y estudiando sus 5 mÃ¡s similares. La elecciÃ³n de palabras\n",
    "#    no debe ser al azar para evitar la apariciÃ³n de tÃ©rminos poco interpretables, elegirlas \"manualmente\".\n",
    "# --------------------------------------\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=5)\n",
    "X_tfidf = tfidf.fit_transform(X_train)\n",
    "terms = np.array(tfidf.get_feature_names_out())\n",
    "# Transponer: filas = palabras, columnas = documentos\n",
    "term_doc = X_tfidf.T\n",
    "sim_matrix = cosine_similarity(term_doc)\n",
    "\n",
    "selected_words = [\"space\", \"computer\", \"car\", \"engine\", \"religion\"]\n",
    "\n",
    "for w in selected_words:\n",
    "    if w not in tfidf.vocabulary_:\n",
    "        print(f\"\\nPalabra '{w}' no encontrada en el vocabulario.\")\n",
    "        continue\n",
    "\n",
    "    idx = tfidf.vocabulary_[w]\n",
    "    sims = sim_matrix[idx]\n",
    "    sims[idx] = 0\n",
    "    top5 = sims.argsort()[-5:][::-1]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"Palabra: '{w}' â€” mÃ¡s similares:\")\n",
    "    for j in top5:\n",
    "        print(f\"  â€¢ {terms[j]:15s}  (similitud = {sims[j]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las similitudes obtenidas son, en general, coherentes y temÃ¡ticamente precisas, especialmente en dominios bien representados en el corpus (como â€œspaceâ€, â€œcarâ€ o â€œreligionâ€).\n",
    "En cambio, ciertos tÃ©rminos mÃ¡s genÃ©ricos o dispersos (â€œcomputerâ€) presentan asociaciones menos claras, lo que podrÃ­a mejorarse, por ejemplo, mediante ponderaciÃ³n TF-IDF, reducciÃ³n de dimensionalidad, o un corpus mÃ¡s especializado.\n",
    "El anÃ¡lisis confirma que la transposiciÃ³n de la matriz y la representaciÃ³n tÃ©rmino-documento permite explorar con Ã©xito relaciones semÃ¡nticas entre palabras a partir de coocurrencias contextuales."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
